{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12333127,"sourceType":"datasetVersion","datasetId":7774525}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\ndef clean_data(df):\n    # Handle missing values and placeholders\n    df = df.replace([\"None\", \"N/A\"], np.nan)\n\n    # Handle -9999.0 values in numeric columns\n    num_cols = df.select_dtypes(include=['number']).columns\n    df[num_cols] = df[num_cols].replace(-9999.0, np.nan)\n\n    # Convert timestamp columns\n    time_cols = [col for col in [\"id4\", \"id5\", \"id12\", \"id13\", \"f370\"] if col in df.columns]\n    for col in time_cols:\n        df[col] = pd.to_datetime(df[col], errors='coerce')\n\n    # Convert key columns to string for consistent merging\n    for col in [\"id2\", \"id3\"]:\n        if col in df.columns:\n            df[col] = df[col].astype(str)\n\n    return df\n\ndef create_features(df, events, transactions, offers):\n    # Ensure merge keys are strings\n    df[\"id2\"] = df[\"id2\"].astype(str)\n    df[\"id3\"] = df[\"id3\"].astype(str)\n    events[\"id2\"] = events[\"id2\"].astype(str)\n    events[\"id3\"] = events[\"id3\"].astype(str)\n    transactions[\"id2\"] = transactions[\"id2\"].astype(str)\n    offers[\"id3\"] = offers[\"id3\"].astype(str)\n\n    # --- 1. Precompute Aggregates ---\n\n    # Customer-level transaction features\n    customer_trans = transactions.groupby('id2').agg(\n        avg_trans_amount=('f367', 'mean'),\n        max_trans_amount=('f367', 'max'),\n        last_trans_date=('f370', 'max'),\n        trans_count=('f370', 'count')\n    ).reset_index()\n\n    # Customer-level event features\n    customer_events = events.groupby('id2').agg(\n        total_impressions=('id4', 'count'),\n        total_clicks=('id7', lambda x: x.notna().sum()),\n        last_impression=('id4', 'max')\n    ).reset_index()\n    customer_events['cust_click_rate'] = customer_events['total_clicks'] / customer_events['total_impressions']\n\n    # Offer-level features\n    offer_features = offers.groupby('id3').agg(\n        discount_rate=('f376', 'mean'),\n        offer_duration=('id13', lambda x: (x.max() - x.min()).days)\n    ).reset_index()\n\n    # Customer-offer interaction features\n    interaction_features = events.groupby(['id2', 'id3']).agg(\n        past_impressions=('id4', 'count'),\n        past_clicks=('id7', lambda x: x.notna().sum()),\n        last_interaction=('id4', 'max')\n    ).reset_index()\n    interaction_features['past_click_rate'] = interaction_features['past_clicks'] / interaction_features['past_impressions']\n\n    # --- 2. Merge Features ---\n    df = df.merge(customer_trans, on='id2', how='left')\n    df = df.merge(customer_events, on='id2', how='left')\n    df = df.merge(offer_features, on='id3', how='left')\n    df = df.merge(interaction_features, on=['id2', 'id3'], how='left')\n\n    # --- 3. Temporal Features ---\n    df['impression_hour'] = df['id4'].dt.hour\n    df['impression_dow'] = df['id4'].dt.dayofweek\n    df['is_weekend'] = df['impression_dow'].isin([5, 6]).astype(int)\n\n    df['days_since_last_trans'] = (df['id4'] - df['last_trans_date']).dt.days\n    df['days_since_last_interaction'] = (df['id4'] - df['last_interaction']).dt.days\n\n    # --- 4. Derived Features ---\n    df['discount_sensitivity'] = df['discount_rate'] * df['cust_click_rate']\n    df['spend_discount_affinity'] = df['avg_trans_amount'] * df['discount_rate']\n\n    # --- 5. Fill Missing Values ---\n    num_cols = ['avg_trans_amount', 'cust_click_rate', 'discount_rate', \n                'past_click_rate', 'days_since_last_trans']\n    for col in num_cols:\n        df[col] = df[col].fillna(df[col].median())\n\n    count_cols = ['past_impressions', 'past_clicks', 'total_impressions']\n    for col in count_cols:\n        df[col] = df[col].fillna(0)\n\n    date_cols = ['last_trans_date', 'last_interaction']\n    old_date = pd.Timestamp('2000-01-01')\n    for col in date_cols:\n        df[col] = df[col].fillna(old_date)\n\n    return df\n\ndef process_data(train_path, test_path, events_path, transactions_path, offers_path):\n    print(\"Loading data...\")\n    train = pd.read_parquet(train_path)\n    test = pd.read_parquet(test_path)\n    events = pd.read_parquet(events_path)\n    transactions = pd.read_parquet(transactions_path)\n    offers = pd.read_parquet(offers_path)\n\n    print(\"Cleaning data...\")\n    train = clean_data(train)\n    test = clean_data(test)\n    events = clean_data(events)\n    transactions = clean_data(transactions)\n    offers = clean_data(offers)\n\n    print(\"Preprocessing events...\")\n    events['click_time'] = events['id7']\n    events['has_clicked'] = events['click_time'].notna().astype(int)\n\n    print(\"Creating features for train set...\")\n    train = create_features(train, events, transactions, offers)\n\n    print(\"Creating features for test set...\")\n    test = create_features(test, events, transactions, offers)\n\n    print(\"Finalizing datasets...\")\n    common_cols = list(set(train.columns) & set(test.columns))\n    train = train[common_cols + ['y']]\n    test = test[common_cols]\n\n    return train, test\n\nif __name__ == \"__main__\":\n    train, test = process_data(\n        train_path=\"/kaggle/input/amex-problem1/train_data.parquet\",\n        test_path=\"/kaggle/input/amex-problem1/test_data.parquet\",\n        events_path=\"/kaggle/input/amex-problem1/add_event.parquet\",\n        transactions_path=\"/kaggle/input/amex-problem1/add_trans.parquet\",\n        offers_path=\"/kaggle/input/amex-problem1/offer_metadata.parquet\"\n    )\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:12:56.160808Z","iopub.execute_input":"2025-07-01T11:12:56.161095Z","iopub.status.idle":"2025-07-01T12:02:12.467496Z","shell.execute_reply.started":"2025-07-01T11:12:56.161074Z","shell.execute_reply":"2025-07-01T12:02:12.466459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\ndef clean_data(df):\n    # Handle missing values and placeholders\n    df = df.replace([\"None\", \"N/A\"], np.nan)\n\n    # Handle -9999.0 values in numeric columns\n    num_cols = df.select_dtypes(include=['number']).columns\n    df[num_cols] = df[num_cols].replace(-9999.0, np.nan)\n\n    # Convert timestamp columns\n    time_cols = [col for col in [\"id4\", \"id5\", \"id12\", \"id13\", \"f370\"] if col in df.columns]\n    for col in time_cols:\n        df[col] = pd.to_datetime(df[col], errors='coerce')\n\n    # Convert key columns to string for consistent merging\n    for col in [\"id2\", \"id3\"]:\n        if col in df.columns:\n            df[col] = df[col].astype(str)\n\n    return df\n\ndef create_features(df, events, transactions, offers):\n    # Ensure merge keys are strings\n    df[\"id2\"] = df[\"id2\"].astype(str)\n    df[\"id3\"] = df[\"id3\"].astype(str)\n    events[\"id2\"] = events[\"id2\"].astype(str)\n    events[\"id3\"] = events[\"id3\"].astype(str)\n    transactions[\"id2\"] = transactions[\"id2\"].astype(str)\n    offers[\"id3\"] = offers[\"id3\"].astype(str)\n\n    # --- 1. Precompute Aggregates ---\n\n    # Customer-level transaction features\n    customer_trans = transactions.groupby('id2').agg(\n        avg_trans_amount=('f367', 'mean'),\n        max_trans_amount=('f367', 'max'),\n        last_trans_date=('f370', 'max'),\n        trans_count=('f370', 'count'),\n        recency_days=('f370', lambda x: (pd.Timestamp('2025-07-02') - x.max()).days),\n        frequency_30d=('f370', lambda x: x.gt(pd.Timestamp('2025-06-02')).sum())\n    ).reset_index()\n\n    # Customer-level event features\n    customer_events = events.groupby('id2').agg(\n        total_impressions=('id4', 'count'),\n        total_clicks=('id7', lambda x: x.notna().sum()),\n        last_impression=('id4', 'max')\n    ).reset_index()\n    customer_events['cust_click_rate'] = customer_events['total_clicks'] / customer_events['total_impressions']\n\n    # Offer-level features\n    offer_features = offers.groupby('id3').agg(\n        discount_rate=('f376', 'mean'),\n        offer_duration=('id13', lambda x: (x.max() - x.min()).days),\n        avg_offer_ctr=('f377', 'mean') if 'f377' in offers.columns else pd.Series(np.nan)\n    ).reset_index()\n\n    # Customer-offer interaction features\n    interaction_features = events.groupby(['id2', 'id3']).agg(\n        past_impressions=('id4', 'count'),\n        past_clicks=('id7', lambda x: x.notna().sum()),\n        last_interaction=('id4', 'max')\n    ).reset_index()\n    interaction_features['past_click_rate'] = interaction_features['past_clicks'] / interaction_features['past_impressions']\n\n    # --- 2. Merge Features ---\n    df = df.merge(customer_trans, on='id2', how='left')\n    df = df.merge(customer_events, on='id2', how='left')\n    df = df.merge(offer_features, on='id3', how='left')\n    df = df.merge(interaction_features, on=['id2', 'id3'], how='left')\n\n    # --- 3. Temporal Features ---\n    df['impression_hour'] = df['id4'].dt.hour\n    df['impression_dow'] = df['id4'].dt.dayofweek\n    df['is_weekend'] = df['impression_dow'].isin([5, 6]).astype(int)\n\n    df['days_since_last_trans'] = (df['id4'] - df['last_trans_date']).dt.days\n    df['days_since_last_interaction'] = (df['id4'] - df['last_interaction']).dt.days\n\n    # --- 4. Derived Features ---\n    df['discount_sensitivity'] = df['discount_rate'] * df['cust_click_rate']\n    df['spend_discount_affinity'] = df['avg_trans_amount'] * df['discount_rate']\n\n    # --- 5. Additional Features ---\n    # Recency, Frequency, Monetary (RFM) features\n    df['recency_days'] = df['recency_days'].fillna(df['recency_days'].median())\n    df['frequency_30d'] = df['frequency_30d'].fillna(0)\n    df['monetary_value'] = df['avg_trans_amount'] * df['trans_count']\n\n    # Fill missing values\n    num_cols = ['avg_trans_amount', 'cust_click_rate', 'discount_rate', \n                'past_click_rate', 'days_since_last_trans', 'recency_days', 'frequency_30d', 'monetary_value']\n    for col in num_cols:\n        df[col] = df[col].fillna(df[col].median())\n\n    count_cols = ['past_impressions', 'past_clicks', 'total_impressions']\n    for col in count_cols:\n        df[col] = df[col].fillna(0)\n\n    date_cols = ['last_trans_date', 'last_interaction']\n    old_date = pd.Timestamp('2000-01-01')\n    for col in date_cols:\n        df[col] = df[col].fillna(old_date)\n\n    return df\n\n\ndef process_data(train_path, test_path, events_path, transactions_path, offers_path):\n    print(\"Loading data...\")\n    train = pd.read_parquet(train_path)\n    test = pd.read_parquet(test_path)\n    events = pd.read_parquet(events_path)\n    transactions = pd.read_parquet(transactions_path)\n    offers = pd.read_parquet(offers_path)\n\n    print(\"Cleaning data...\")\n    train = clean_data(train)\n    test = clean_data(test)\n    events = clean_data(events)\n    transactions = clean_data(transactions)\n    offers = clean_data(offers)\n\n    print(\"Preprocessing events...\")\n    events['click_time'] = events['id7']\n    events['has_clicked'] = events['click_time'].notna().astype(int)\n\n    print(\"Creating features for train set...\")\n    train = create_features(train, events, transactions, offers)\n\n    print(\"Creating features for test set...\")\n    test = create_features(test, events, transactions, offers)\n\n    print(\"Finalizing datasets...\")\n    common_cols = list(set(train.columns) & set(test.columns))\n    train = train[common_cols + ['y']]\n    test = test[common_cols]\n\n    return train, test\n\nif __name__ == \"__main__\":\n    train, test = process_data(\n        train_path=\"/kaggle/input/amex-problem1/train_data.parquet\",\n        test_path=\"/kaggle/input/amex-problem1/test_data.parquet\",\n        events_path=\"/kaggle/input/amex-problem1/add_event.parquet\",\n        transactions_path=\"/kaggle/input/amex-problem1/add_trans.parquet\",\n        offers_path=\"/kaggle/input/amex-problem1/offer_metadata.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:58:55.239432Z","iopub.execute_input":"2025-07-02T14:58:55.240105Z","iopub.status.idle":"2025-07-02T15:49:58.407075Z","shell.execute_reply.started":"2025-07-02T14:58:55.240078Z","shell.execute_reply":"2025-07-02T15:49:58.405951Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nCleaning data...\nPreprocessing events...\nCreating features for train set...\nCreating features for test set...\nFinalizing datasets...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.utils import shuffle\nimport lightgbm as lgb\nfrom lightgbm import early_stopping, log_evaluation\nfrom tqdm import tqdm\n\n# === Custom MAP@7 ===\ndef calculate_map7(y_true, y_pred, id2_val, k=7):\n    df = pd.DataFrame({'id2': id2_val, 'true': y_true, 'pred': y_pred})\n    df['rank'] = df.groupby('id2')['pred'].rank(method='first', ascending=False)\n    top_k = df[df['rank'] <= k].copy()\n    top_k['correct'] = top_k['true'].astype(bool)\n    top_k['cumulative_correct'] = top_k.groupby('id2')['correct'].cumsum()\n    top_k['precision_at_k'] = top_k['cumulative_correct'] / top_k['rank']\n    ap_per_customer = top_k.groupby('id2')['precision_at_k'] \\\n        .apply(lambda x: x[x > 0].mean() if any(x > 0) else 0)\n    return ap_per_customer.mean()\n\n# === Data Prep ===\nX = train.drop(['y', 'id1', 'id4', 'id5'], axis=1)\ny = train['y'].astype(int)\nid2_series = train['id2'].astype(str)\n\nX = X.apply(pd.to_numeric, errors='coerce').fillna(np.nan)\nX = X.select_dtypes(include='number')\n\n# === Final Fold Validation ===\ntscv = TimeSeriesSplit(n_splits=5)\nsplits = list(tscv.split(X))\ntrain_idx, val_idx = splits[-1]\n\nX_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\ny_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\nid2_val = id2_series.iloc[val_idx].values\n\n# === Shuffle ===\nX_train, y_train = shuffle(X_train, y_train, random_state=42)\n\n# === LightGBM ===\nlgb_model = lgb.LGBMClassifier(\n    objective='binary',\n    learning_rate=0.03,\n    num_leaves=128,\n    max_depth=9,\n    subsample=0.85,\n    colsample_bytree=0.85,\n    reg_alpha=0.3,\n    reg_lambda=0.3,\n    n_estimators=5000,\n    random_state=42\n)\nlgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='binary_logloss',\n    callbacks=[\n        early_stopping(stopping_rounds=20),\n        log_evaluation(100)\n    ]\n)\nbest_iter_lgb = lgb_model.best_iteration_\nval_pred_lgb = lgb_model.predict_proba(X_val, num_iteration=best_iter_lgb)[:, 1]\nprint(\"MAP@7 LGB:\", calculate_map7(y_val, val_pred_lgb, id2_val))","metadata":{"execution":{"iopub.status.busy":"2025-07-02T16:07:38.878247Z","iopub.execute_input":"2025-07-02T16:07:38.879339Z","iopub.status.idle":"2025-07-02T16:30:56.460619Z","shell.execute_reply.started":"2025-07-02T16:07:38.879307Z","shell.execute_reply":"2025-07-02T16:30:56.459112Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 31192, number of negative: 610612\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.131444 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 48004\n[LightGBM] [Info] Number of data points in the train set: 641804, number of used features: 359\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048601 -> initscore=-2.974300\n[LightGBM] [Info] Start training from score -2.974300\nTraining until validation scores don't improve for 20 rounds\n[100]\tvalid_0's binary_logloss: 0.106565\n[200]\tvalid_0's binary_logloss: 0.105927\nEarly stopping, best iteration is:\n[180]\tvalid_0's binary_logloss: 0.105848\nMAP@7 LGB: 0.05499165820366656\nMAP@7 CatBoost: 0.0556849168357849\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"MAP@7 XGBoost: 0.05568497637447911\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"lgb_params = lgb_model.get_params()\nlgb_params['n_estimators'] = best_iter_lgb\n\nlgb_model_final = lgb.LGBMClassifier(**lgb_params)\nlgb_model_final.fit(X, y)\n\ncat_params = cat_model.get_params()\ncat_params['iterations'] = cat_model.get_best_iteration()\n\ncat_model_final = CatBoostClassifier(**cat_params)\ncat_model_final.fit(X, y, verbose=0)\n\nxgb_params = xgb_model.get_xgb_params()\nxgb_params['n_estimators'] = best_iter_xgb\n\nxgb_model_final = xgb.XGBClassifier(**xgb_params)\nxgb_model_final.fit(X, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T16:37:01.047986Z","iopub.execute_input":"2025-07-02T16:37:01.048469Z","iopub.status.idle":"2025-07-02T16:48:55.362973Z","shell.execute_reply.started":"2025-07-02T16:37:01.048387Z","shell.execute_reply":"2025-07-02T16:48:55.361925Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 37051, number of negative: 733113\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.808288 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 48272\n[LightGBM] [Info] Number of data points in the train set: 770164, number of used features: 359\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048108 -> initscore=-2.985005\n[LightGBM] [Info] Start training from score -2.985005\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.85, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=9,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=154,\n              n_jobs=None, num_parallel_tree=None, random_state=42, ...)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.85, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=9,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=154,\n              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.85, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=9,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=154,\n              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# === Final Predictions ===\nX_test = test[X.columns].apply(pd.to_numeric, errors='coerce').fillna(np.nan)\nlgb_preds = lgb_model_final.predict_proba(X_test)[:, 1]\ncat_preds = cat_model_final.predict_proba(X_test)[:, 1]\nxgb_preds = xgb_model_final.predict_proba(X_test)[:, 1]\n\ntest_preds = (lgb_preds + cat_preds + xgb_preds) / 3\ntest['pred'] = test_preds + 1e-6 * np.random.rand(len(test))  # tie-breaking noise\n\n# === Submission File ===\nrequired_cols = ['id1', 'id2', 'id3', 'id5', 'pred']\nfor col in ['id1', 'id2', 'id3', 'id5']:\n    test[col] = test[col].astype(str).str.strip()\n\nsubmission = test[required_cols].copy()\nsubmission.to_csv(\"r2_final_submission_ensemble.csv\", index=False)\nprint(\"✅ Final submission shape:\", submission.shape) # should be (369301,5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T16:49:44.839233Z","iopub.execute_input":"2025-07-02T16:49:44.839599Z","iopub.status.idle":"2025-07-02T16:50:37.221975Z","shell.execute_reply.started":"2025-07-02T16:49:44.839574Z","shell.execute_reply":"2025-07-02T16:50:37.220982Z"}},"outputs":[{"name":"stdout","text":"✅ Final submission shape: (369301, 5)\n","output_type":"stream"}],"execution_count":13}]}